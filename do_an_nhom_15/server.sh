python -m llama_cpp.server --model "./Models/Llama-3.2-1B-Instruct-Q6_K_L.gguf" --chat_format chatml --n_gpu_layers 1\